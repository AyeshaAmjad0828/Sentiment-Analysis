{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26d21061",
   "metadata": {},
   "source": [
    "# Sentiment Analysis by Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b3af7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: threadpoolctl==3.1.0 in g:\\anaconda\\lib\\site-packages (3.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -cipy (g:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (g:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (g:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (g:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (g:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (g:\\anaconda\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in g:\\anaconda\\lib\\site-packages (1.4.0)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.4.1.post1-cp310-cp310-win_amd64.whl (10.6 MB)\n",
      "     ---------------------------------------- 10.6/10.6 MB 3.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in g:\\anaconda\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in g:\\anaconda\\lib\\site-packages (from scikit-learn) (1.24.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in g:\\anaconda\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in g:\\anaconda\\lib\\site-packages (from scikit-learn) (1.12.0)\n",
      "Installing collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.4.0\n",
      "    Uninstalling scikit-learn-1.4.0:\n",
      "      Successfully uninstalled scikit-learn-1.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -cipy (g:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (g:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (g:\\anaconda\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -cipy (g:\\anaconda\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'G:\\\\Anaconda\\\\Lib\\\\site-packages\\\\~klearn\\\\.libs\\\\msvcp140.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "WARNING: Ignoring invalid distribution -cipy (g:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (g:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (g:\\anaconda\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install threadpoolctl==3.1.0\n",
    "!pip install -U scikit-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4829ae07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import re\n",
    "\n",
    "# Download WordNet and SentiWordNet data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6cb9e2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMDb dataset\n",
    "imdb_data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        # Remove HTML tags and special characters\n",
    "        text = re.sub('<[^<]+?>', ' ', text)\n",
    "        text = re.sub(r'\\W', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.lower()\n",
    "        # Tokenization\n",
    "        tokens = word_tokenize(text)\n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        # POS Tagging\n",
    "        tokens = pos_tag(tokens)\n",
    "        return ' '.join([word for word, tag in tokens if tag.startswith('N')])  # Keep only nouns\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        print(\"Input Text:\", text)\n",
    "        return \"\"  # Return empty string in case of error\n",
    "\n",
    "\n",
    "imdb_data['preprocessed_text'] = imdb_data['review'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b203584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMDb dataset\n",
    "imdb_data_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        # Remove HTML tags and special characters\n",
    "        text = re.sub('<[^<]+?>', ' ', text)\n",
    "        text = re.sub(r'\\W', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.lower()\n",
    "        # Tokenization\n",
    "        tokens = word_tokenize(text)\n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        # POS Tagging\n",
    "        tokens = pos_tag(tokens)\n",
    "        return ' '.join([word for word, tag in tokens if tag.startswith('N')])  # Keep only nouns\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        print(\"Input Text:\", text)\n",
    "        return \"\"  # Return empty string in case of error\n",
    "\n",
    "\n",
    "imdb_data_test['preprocessed_text'] = imdb_data_test['review'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d96ce9",
   "metadata": {},
   "source": [
    "# Experiment no 1: Tfidf vectorizer(max features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "78b88703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.55085\n",
      "Random Forest Accuracy: 0.54825\n",
      "k-NN Accuracy: 0.49715\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the text data\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # You can experiment with different parameters\n",
    "X_train = vectorizer.fit_transform(imdb_data['preprocessed_text'])\n",
    "y_train = imdb_data['sentiment']\n",
    "\n",
    "X_test = vectorizer.fit_transform(imdb_data_test['preprocessed_text'])\n",
    "y_test = imdb_data_test['sentiment']\n",
    "\n",
    "\n",
    "# Train machine learning models\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=2)\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model performance\n",
    "nb_pred = nb_classifier.predict(X_test)\n",
    "rf_pred = rf_classifier.predict(X_test)\n",
    "knn_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "nb_accuracy1 = accuracy_score(y_test, nb_pred)\n",
    "rf_accuracy1 = accuracy_score(y_test, rf_pred)\n",
    "knn_accuracy1 = accuracy_score(y_test, knn_pred)\n",
    "\n",
    "print(\"Naive Bayes Accuracy:\", nb_accuracy1)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy1)\n",
    "print(\"k-NN Accuracy:\", knn_accuracy1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d91094",
   "metadata": {},
   "source": [
    "# Experiment no 2: Tfidf vectorizer(max features=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db8837a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.53215\n",
      "Random Forest Accuracy: 0.4852\n",
      "k-NN Accuracy: 0.49685\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = TfidfVectorizer(max_features=6000)  # You can experiment with different parameters\n",
    "X_train = vectorizer.fit_transform(imdb_data['preprocessed_text'])\n",
    "y_train = imdb_data['sentiment']\n",
    "\n",
    "X_test = vectorizer.fit_transform(imdb_data_test['preprocessed_text'])\n",
    "y_test = imdb_data_test['sentiment']\n",
    "\n",
    "# Train machine learning models\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=2)\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model performance\n",
    "nb_pred = nb_classifier.predict(X_test)\n",
    "rf_pred = rf_classifier.predict(X_test)\n",
    "knn_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "nb_accuracy2 = accuracy_score(y_test, nb_pred)\n",
    "rf_accuracy2 = accuracy_score(y_test, rf_pred)\n",
    "knn_accuracy2 = accuracy_score(y_test, knn_pred)\n",
    "\n",
    "print(\"Naive Bayes Accuracy:\", nb_accuracy2)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy2)\n",
    "print(\"k-NN Accuracy:\", knn_accuracy2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d9dc4b",
   "metadata": {},
   "source": [
    "# Experiment no 3: Tfidf vectorizer(max features=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8d4d8753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.5289\n",
      "Random Forest Accuracy: 0.5006\n",
      "k-NN Accuracy: 0.4965\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = TfidfVectorizer(max_features=4000)  # You can experiment with different parameters\n",
    "X_train = vectorizer.fit_transform(imdb_data['preprocessed_text'])\n",
    "y_train = imdb_data['sentiment']\n",
    "\n",
    "X_test = vectorizer.fit_transform(imdb_data_test['preprocessed_text'])\n",
    "y_test = imdb_data_test['sentiment']\n",
    "\n",
    "# Train machine learning models\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=2)\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model performance\n",
    "nb_pred = nb_classifier.predict(X_test)\n",
    "rf_pred = rf_classifier.predict(X_test)\n",
    "knn_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "nb_accuracy3 = accuracy_score(y_test, nb_pred)\n",
    "rf_accuracy3 = accuracy_score(y_test, rf_pred)\n",
    "knn_accuracy3 = accuracy_score(y_test, knn_pred)\n",
    "\n",
    "print(\"Naive Bayes Accuracy:\", nb_accuracy3)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy3)\n",
    "print(\"k-NN Accuracy:\", knn_accuracy3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f99a36e",
   "metadata": {},
   "source": [
    "# Experiment no 4: Tfidf vectorizer(max features=4000,ngram(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d716f71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.53115\n",
      "Random Forest Accuracy: 0.51385\n",
      "k-NN Accuracy: 0.50825\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the TfidfVectorizer with various options\n",
    "vectorizer = TfidfVectorizer(max_features=5000,  # Limits the number of features\n",
    "                             ngram_range=(1, 2),  # Considers unigrams and bigrams\n",
    "                             max_df=0.8,  # Ignores terms with document frequency > 80%\n",
    "                             min_df=5,  # Ignores terms with document frequency < 5\n",
    "                             sublinear_tf=True,  # Scales term frequency logarithmically\n",
    "                             smooth_idf=True,  # Adds a smoothing term to IDF\n",
    "                             norm='l2'  # Normalizes the TF-IDF vectors using L2 norm\n",
    "                            )\n",
    "\n",
    "X = vectorizer.fit_transform(imdb_data['preprocessed_text'])\n",
    "X_train = vectorizer.fit_transform(imdb_data['preprocessed_text'])\n",
    "y_train = imdb_data['sentiment']\n",
    "\n",
    "X_test = vectorizer.fit_transform(imdb_data_test['preprocessed_text'])\n",
    "y_test = imdb_data_test['sentiment']\n",
    "\n",
    "# Train machine learning models\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model performance\n",
    "nb_pred = nb_classifier.predict(X_test)\n",
    "rf_pred = rf_classifier.predict(X_test)\n",
    "knn_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "nb_accuracy4 = accuracy_score(y_test, nb_pred)\n",
    "rf_accuracy4 = accuracy_score(y_test, rf_pred)\n",
    "knn_accuracy4 = accuracy_score(y_test, knn_pred)\n",
    "\n",
    "print(\"Naive Bayes Accuracy:\", nb_accuracy4)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy4)\n",
    "print(\"k-NN Accuracy:\", knn_accuracy4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2728d2ff",
   "metadata": {},
   "source": [
    "# Experiment no 5: CountVectorizer(max_features=5000, ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8ee23bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.57165\n",
      "Random Forest Accuracy: 0.52355\n",
      "k-NN Accuracy: 0.49585\n"
     ]
    }
   ],
   "source": [
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=5000,  # Limits the number of features\n",
    "                             ngram_range=(1, 2),  # Considers unigrams and bigrams\n",
    "                             stop_words='english',  # Removes English stopwords\n",
    "                             max_df=0.8,  # Ignores terms with document frequency > 80%\n",
    "                             min_df=5  # Ignores terms with document frequency < 5\n",
    "                            )\n",
    "\n",
    "\n",
    "X = vectorizer.fit_transform(imdb_data['preprocessed_text'])\n",
    "X_train = vectorizer.fit_transform(imdb_data['preprocessed_text'])\n",
    "y_train = imdb_data['sentiment']\n",
    "\n",
    "X_test = vectorizer.fit_transform(imdb_data_test['preprocessed_text'])\n",
    "y_test = imdb_data_test['sentiment']\n",
    "\n",
    "# Train machine learning models\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model performance\n",
    "nb_pred = nb_classifier.predict(X_test)\n",
    "rf_pred = rf_classifier.predict(X_test)\n",
    "knn_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "nb_accuracy5 = accuracy_score(y_test, nb_pred)\n",
    "rf_accuracy5 = accuracy_score(y_test, rf_pred)\n",
    "knn_accuracy5 = accuracy_score(y_test, knn_pred)\n",
    "\n",
    "print(\"Naive Bayes Accuracy:\", nb_accuracy5)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy5)\n",
    "print(\"k-NN Accuracy:\", knn_accuracy5)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfa085b",
   "metadata": {},
   "source": [
    "# Experiment no 6: Count vectorizer(max features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c3126f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.54585\n",
      "Random Forest Accuracy: 0.5309\n",
      "k-NN Accuracy: 0.52245\n"
     ]
    }
   ],
   "source": [
    "imdb_data['preprocessed_text'] = imdb_data['review'].apply(preprocess_text)\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=5000 # Limits the number of features\n",
    "                    \n",
    "                            )\n",
    "\n",
    "\n",
    "X_train = vectorizer.fit_transform(imdb_data['preprocessed_text'])\n",
    "y_train = imdb_data['sentiment']\n",
    "\n",
    "X_test = vectorizer.fit_transform(imdb_data_test['preprocessed_text'])\n",
    "y_test = imdb_data_test['sentiment']\n",
    "\n",
    "# Train machine learning models\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model performance\n",
    "nb_pred = nb_classifier.predict(X_test)\n",
    "rf_pred = rf_classifier.predict(X_test)\n",
    "knn_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "nb_accuracy6 = accuracy_score(y_test, nb_pred)\n",
    "rf_accuracy6 = accuracy_score(y_test, rf_pred)\n",
    "knn_accuracy6 = accuracy_score(y_test, knn_pred)\n",
    "\n",
    "print(\"Naive Bayes Accuracy:\", nb_accuracy6)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy6)\n",
    "print(\"k-NN Accuracy:\", knn_accuracy6)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af5c1ec",
   "metadata": {},
   "source": [
    "# Experiment no 7:Tfidf(max features=5000, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ed04dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.56135\n",
      "Random Forest Accuracy: 0.52415\n",
      "k-NN Accuracy: 0.50605\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TfidfVectorizer with various options\n",
    "vectorizer = TfidfVectorizer(max_features=5000,  # Limits the number of features\n",
    "                             ngram_range=(1, 3),  # Considers unigrams and bigrams\n",
    "                             max_df=0.8,  # Ignores terms with document frequency > 80%\n",
    "                             min_df=5,  # Ignores terms with document frequency < 5\n",
    "                             sublinear_tf=True,  # Scales term frequency logarithmically\n",
    "                             smooth_idf=True,  # Adds a smoothing term to IDF\n",
    "                             norm='l2'  # Normalizes the TF-IDF vectors using L2 norm\n",
    "                            )\n",
    "\n",
    "X_train = vectorizer.fit_transform(imdb_data['preprocessed_text'])\n",
    "y_train = imdb_data['sentiment']\n",
    "\n",
    "X_test = vectorizer.fit_transform(imdb_data_test['preprocessed_text'])\n",
    "y_test = imdb_data_test['sentiment']\n",
    "\n",
    "# Train machine learning models\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model performance\n",
    "nb_pred = nb_classifier.predict(X_test)\n",
    "rf_pred = rf_classifier.predict(X_test)\n",
    "knn_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "nb_accuracy7 = accuracy_score(y_test, nb_pred)\n",
    "rf_accuracy7 = accuracy_score(y_test, rf_pred)\n",
    "knn_accuracy7 = accuracy_score(y_test, knn_pred)\n",
    "\n",
    "print(\"Naive Bayes Accuracy:\", nb_accuracy7)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy7)\n",
    "print(\"k-NN Accuracy:\", knn_accuracy7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "89c80162",
   "metadata": {},
   "outputs": [],
   "source": [
    "Results=pd.DataFrame({\"Settings\":['Tfidf vectorizer(max features=5000,ngram_range=(1,1))','Tfidf vectorizer(max features=6000),ngram_range=(1,1)','Tfidf vectorizer(max features=4000,,ngram_range=(1,1))','Tfidf vectorizer(max features=5000,,ngram_range=(1,2))','Count vectorizer(max features=5000,ngram_range=(1,1))','Count vectorizer(max features=5000,ngram_range=(1, 2))','Tfidf vectorizer(max features=5000, ngram_range=(1,3))'],'Naive Bayes Accuracy':[nb_accuracy1,nb_accuracy2,nb_accuracy3,nb_accuracy4,nb_accuracy5,nb_accuracy6,nb_accuracy7],\"Random Forest Accuracy\":[ rf_accuracy1, rf_accuracy2, rf_accuracy3, rf_accuracy4, rf_accuracy5, rf_accuracy6,rf_accuracy7],\"k-NN Accuracy\":[knn_accuracy1,knn_accuracy2,knn_accuracy3,knn_accuracy4,knn_accuracy5,knn_accuracy6,knn_accuracy7]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4b45a7f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_b9720 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_b9720 tr:nth-of-type(odd) {\n",
       "  background-color: rgba(0,0,0,0.05);\n",
       "}\n",
       "#T_b9720 tr:hover {\n",
       "  background-color: rgba(0,0,0,0.1);\n",
       "}\n",
       "#T_b9720_row0_col0, #T_b9720_row0_col1, #T_b9720_row0_col2, #T_b9720_row0_col3, #T_b9720_row1_col0, #T_b9720_row1_col1, #T_b9720_row1_col2, #T_b9720_row1_col3, #T_b9720_row2_col0, #T_b9720_row2_col1, #T_b9720_row2_col2, #T_b9720_row2_col3, #T_b9720_row3_col0, #T_b9720_row3_col1, #T_b9720_row3_col2, #T_b9720_row3_col3, #T_b9720_row4_col0, #T_b9720_row4_col1, #T_b9720_row4_col2, #T_b9720_row4_col3, #T_b9720_row5_col0, #T_b9720_row5_col1, #T_b9720_row5_col2, #T_b9720_row5_col3, #T_b9720_row6_col0, #T_b9720_row6_col1, #T_b9720_row6_col2, #T_b9720_row6_col3 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_b9720\">\n",
       "  <caption>Results Dashboard</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b9720_level0_col0\" class=\"col_heading level0 col0\" >Settings</th>\n",
       "      <th id=\"T_b9720_level0_col1\" class=\"col_heading level0 col1\" >Naive Bayes Accuracy</th>\n",
       "      <th id=\"T_b9720_level0_col2\" class=\"col_heading level0 col2\" >Random Forest Accuracy</th>\n",
       "      <th id=\"T_b9720_level0_col3\" class=\"col_heading level0 col3\" >k-NN Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b9720_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_b9720_row0_col0\" class=\"data row0 col0\" >Tfidf vectorizer(max features=5000,ngram_range=(1,1))</td>\n",
       "      <td id=\"T_b9720_row0_col1\" class=\"data row0 col1\" >0.550850</td>\n",
       "      <td id=\"T_b9720_row0_col2\" class=\"data row0 col2\" >0.548250</td>\n",
       "      <td id=\"T_b9720_row0_col3\" class=\"data row0 col3\" >0.497150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9720_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_b9720_row1_col0\" class=\"data row1 col0\" >Tfidf vectorizer(max features=6000),ngram_range=(1,1)</td>\n",
       "      <td id=\"T_b9720_row1_col1\" class=\"data row1 col1\" >0.532150</td>\n",
       "      <td id=\"T_b9720_row1_col2\" class=\"data row1 col2\" >0.485200</td>\n",
       "      <td id=\"T_b9720_row1_col3\" class=\"data row1 col3\" >0.496850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9720_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_b9720_row2_col0\" class=\"data row2 col0\" >Tfidf vectorizer(max features=4000,,ngram_range=(1,1))</td>\n",
       "      <td id=\"T_b9720_row2_col1\" class=\"data row2 col1\" >0.528900</td>\n",
       "      <td id=\"T_b9720_row2_col2\" class=\"data row2 col2\" >0.500600</td>\n",
       "      <td id=\"T_b9720_row2_col3\" class=\"data row2 col3\" >0.496500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9720_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_b9720_row3_col0\" class=\"data row3 col0\" >Tfidf vectorizer(max features=5000,,ngram_range=(1,2))</td>\n",
       "      <td id=\"T_b9720_row3_col1\" class=\"data row3 col1\" >0.531150</td>\n",
       "      <td id=\"T_b9720_row3_col2\" class=\"data row3 col2\" >0.513850</td>\n",
       "      <td id=\"T_b9720_row3_col3\" class=\"data row3 col3\" >0.508250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9720_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_b9720_row4_col0\" class=\"data row4 col0\" >Count vectorizer(max features=5000,ngram_range=(1,1))</td>\n",
       "      <td id=\"T_b9720_row4_col1\" class=\"data row4 col1\" >0.571650</td>\n",
       "      <td id=\"T_b9720_row4_col2\" class=\"data row4 col2\" >0.523550</td>\n",
       "      <td id=\"T_b9720_row4_col3\" class=\"data row4 col3\" >0.495850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9720_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_b9720_row5_col0\" class=\"data row5 col0\" >Count vectorizer(max features=5000,ngram_range=(1, 2))</td>\n",
       "      <td id=\"T_b9720_row5_col1\" class=\"data row5 col1\" >0.545850</td>\n",
       "      <td id=\"T_b9720_row5_col2\" class=\"data row5 col2\" >0.530900</td>\n",
       "      <td id=\"T_b9720_row5_col3\" class=\"data row5 col3\" >0.522450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9720_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_b9720_row6_col0\" class=\"data row6 col0\" >Tfidf vectorizer(max features=5000, ngram_range=(1,3))</td>\n",
       "      <td id=\"T_b9720_row6_col1\" class=\"data row6 col1\" >0.561350</td>\n",
       "      <td id=\"T_b9720_row6_col2\" class=\"data row6 col2\" >0.524150</td>\n",
       "      <td id=\"T_b9720_row6_col3\" class=\"data row6 col3\" >0.506050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1ecab975b70>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Apply styles to the DataFrame\n",
    "styled_results = Results.style \\\n",
    "    .set_properties(**{'text-align': 'left'}) \\\n",
    "    .set_table_styles([\n",
    "        {'selector': 'th', 'props': [('text-align', 'left')]},  # Left-align column headers\n",
    "        {'selector': 'tr:nth-of-type(odd)', 'props': [('background-color', 'rgba(0,0,0,0.05)')]},  # Add alternating row colors\n",
    "        {'selector': 'tr:hover', 'props': [('background-color', 'rgba(0,0,0,0.1)')]}  # Highlight row on hover\n",
    "    ]) \\\n",
    "    .set_caption(\"Results Dashboard\")  # Add a caption\n",
    "\n",
    "# Print the styled DataFrame\n",
    "styled_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ed6094",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
