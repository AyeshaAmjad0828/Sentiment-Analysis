{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0626dd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments, GPT2Config\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2f8cfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maamjad\u001b[0m (\u001b[33mtab-llm-finetuning\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/AsteraTransformers2/experiments/ayesha/wandb/run-20240304_083926-5gfx2m5r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tab-llm-finetuning/gpt2-sentiment-analysis/runs/5gfx2m5r' target=\"_blank\">ayesha3</a></strong> to <a href='https://wandb.ai/tab-llm-finetuning/gpt2-sentiment-analysis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tab-llm-finetuning/gpt2-sentiment-analysis' target=\"_blank\">https://wandb.ai/tab-llm-finetuning/gpt2-sentiment-analysis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tab-llm-finetuning/gpt2-sentiment-analysis/runs/5gfx2m5r' target=\"_blank\">https://wandb.ai/tab-llm-finetuning/gpt2-sentiment-analysis/runs/5gfx2m5r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/tab-llm-finetuning/gpt2-sentiment-analysis/runs/5gfx2m5r?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f46cfecb410>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"gpt2-sentiment-analysis\", name='ayesha3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dd552f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "540694fd-ead6-4590-8be0-a419e64d4aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review', 'sentiment'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "016ea09d-8e11-4431-8688-a7ed5c5bda80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoviewReviewData(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe3ac9f8-506c-4266-beef-0d065630c3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afeb6a70-9d6a-4175-832c-1ab6316de5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can tokenize the data with padding\n",
    "train_encodings = tokenizer(train_df['review'].tolist(), truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(test_df['review'].tolist(), truncation=True, padding=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1db88cfb-7454-4ddf-8318-24192e2153c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to numeric\n",
    "train_labels = train_df['sentiment'].map({'positive': 1, 'negative': 0}).tolist()\n",
    "test_labels = test_df['sentiment'].map({'positive': 1, 'negative': 0}).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b79b93d-6246-4c1c-9db6-0184f29e35b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "train_dataset = MoviewReviewData(train_encodings, train_labels)\n",
    "test_dataset = MoviewReviewData(test_encodings, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97ce0ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = GPT2Config.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id, num_labels=2)\n",
    "model = GPT2ForSequenceClassification.from_pretrained(model_name, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0553c1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb883d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training arguments...\n",
      "Start training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6701' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6701/7500 3:32:42 < 25:22, 0.52 it/s, Epoch 3.57/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.526900</td>\n",
       "      <td>0.646496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.438400</td>\n",
       "      <td>0.332753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.299700</td>\n",
       "      <td>0.246806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.306000</td>\n",
       "      <td>0.304943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.303300</td>\n",
       "      <td>0.238848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.296400</td>\n",
       "      <td>0.230130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.283700</td>\n",
       "      <td>0.232711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.272400</td>\n",
       "      <td>0.236149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.243500</td>\n",
       "      <td>0.213369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.229200</td>\n",
       "      <td>0.238643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.247700</td>\n",
       "      <td>0.231199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.244900</td>\n",
       "      <td>0.201868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.255700</td>\n",
       "      <td>0.219529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.229200</td>\n",
       "      <td>0.213486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.229700</td>\n",
       "      <td>0.262024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.256300</td>\n",
       "      <td>0.187008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.219300</td>\n",
       "      <td>0.236464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.218700</td>\n",
       "      <td>0.186600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.190800</td>\n",
       "      <td>0.234815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.182500</td>\n",
       "      <td>0.220289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.187800</td>\n",
       "      <td>0.199597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.150800</td>\n",
       "      <td>0.302697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.174200</td>\n",
       "      <td>0.341333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.193800</td>\n",
       "      <td>0.212782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.187200</td>\n",
       "      <td>0.188551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.177200</td>\n",
       "      <td>0.188560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.162800</td>\n",
       "      <td>0.245818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.195600</td>\n",
       "      <td>0.232526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.178200</td>\n",
       "      <td>0.211979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.181500</td>\n",
       "      <td>0.202118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.165800</td>\n",
       "      <td>0.207386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.141900</td>\n",
       "      <td>0.230560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.165700</td>\n",
       "      <td>0.227690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.151600</td>\n",
       "      <td>0.224609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.136300</td>\n",
       "      <td>0.239512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.192200</td>\n",
       "      <td>0.178830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.154800</td>\n",
       "      <td>0.195519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.259427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.118900</td>\n",
       "      <td>0.261158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.109100</td>\n",
       "      <td>0.246268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.094600</td>\n",
       "      <td>0.313206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.114300</td>\n",
       "      <td>0.294308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.077500</td>\n",
       "      <td>0.289919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.127400</td>\n",
       "      <td>0.249531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.103900</td>\n",
       "      <td>0.275174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.094300</td>\n",
       "      <td>0.255962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.121100</td>\n",
       "      <td>0.274886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.101400</td>\n",
       "      <td>0.255402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.123100</td>\n",
       "      <td>0.266238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.117700</td>\n",
       "      <td>0.241336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.096000</td>\n",
       "      <td>0.267690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.112800</td>\n",
       "      <td>0.268086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.103400</td>\n",
       "      <td>0.293346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.111900</td>\n",
       "      <td>0.267766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.118000</td>\n",
       "      <td>0.257240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.101500</td>\n",
       "      <td>0.293174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.069200</td>\n",
       "      <td>0.290223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.041100</td>\n",
       "      <td>0.345860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.069900</td>\n",
       "      <td>0.327912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.337742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.069300</td>\n",
       "      <td>0.326303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.052100</td>\n",
       "      <td>0.321294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.106600</td>\n",
       "      <td>0.277736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.073200</td>\n",
       "      <td>0.276044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.077100</td>\n",
       "      <td>0.296980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.050400</td>\n",
       "      <td>0.312219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='606' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 606/1250 01:15 < 01:19, 8.05 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Loading training arguments...\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='data/result',  # output directory for model and checkpoints\n",
    "    num_train_epochs=4,           # total number of training epochs\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=16,   # batch size per device during training\n",
    "    per_device_eval_batch_size=16,    # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='data/logs',            # directory for storing logs\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"wandb\",                # Disables wandb logging\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "print(\"Start training...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"Saving finetuned gpt2...\")\n",
    "model.save_pretrained('data/result/fine_tuned_gpt2_model2')\n",
    "tokenizer.save_pretrained('data/result/fine_tuned_gpt2_model2')\n",
    "\n",
    "print(\"Start testing...\")\n",
    "# eval mode on model\n",
    "trainer.evaluate()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aedb00-fcfc-4b52-8051-0f71c5187a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df['review']\n",
    "y_train = train_df['sentiment']\n",
    "X_test = test_df['review']\n",
    "y_test = test_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3000cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2ForSequenceClassification.from_pretrained('data/result/fine_tuned_gpt2_model2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('data/result/fine_tuned_gpt2_model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e3636e-b455-4510-a947-c35aa19eced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_numeric = y_train.map({'positive': 1, 'negative': 0}).astype(int)\n",
    "y_test_numeric = y_test.map({'positive': 1, 'negative': 0}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21af4680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#test_encodings = tokenizer(test_data['review'].tolist(), truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Convert labels to a tensor\n",
    "labels = torch.tensor(y_test_numeric)\n",
    "\n",
    "# Create a DataLoader for the test set (optional, for batch processing)\n",
    "test_dataset = MoviewReviewData(test_encodings, labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "    accuracy = (correct_predictions / total_predictions) * 100\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = evaluate(model, test_loader)\n",
    "print(f\"Model accuracy on the test set: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f0dac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fe93f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e2b103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182fd9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fc8cad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b0d064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53998b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73562579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40da8291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18419c94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
